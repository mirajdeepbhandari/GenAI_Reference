# Fine-Tuning Large Language Models (LLMs)

**Welcome! Here, you'll discover how to fine-tune any LLM for your specific needs using your own dataset. Whether it's for your shop, office, school, or any other business, you can create a custom Q&A chatbot tailored to your requirements. Let's dive into how you can structure a chat template and fine-tune an LLM model using Hugging Face Transformers.**



## Fine-Tuning Models (Train on Custom DataSet Code)

| File                                        | Resources |
| ------------------------------------------- | :-------: |
| 1) Fine Tune Llama 2 Chat Model (Code)      | [ðŸ”—](#)  |
| 2) Fine Tune Gemma 2b-it Chat Model (Code)  | [ðŸ”—](#)  |


### Fine Tuning Llama 2 Steps:

| Steps                                                       | Resources |
| ----------------------------------------------------------- | :-------: |
| 1) Prepare the txt file containing questions and answers    | [ðŸ”—](#)  Example of dataset |
| 2) Format the txt file into Llama Accepting Template        | [ðŸ”—](#) Code for formation of data, [ðŸ”—](#)  Example of Llama accepting template data |
| 3) Upload the Llama Accepting Csv file into Hugging Face dataset repo | - |
| 4) Run the Jupyter Code for Tuning the Llama 2 with just prepared dataset | [ðŸ”—](#) Train Code |
| 5) Test the Tuned model                                      | [ðŸ”—](#) Test Code |


# Contributing
We welcome contributions! If you'd like to contribute to this project, whether it's by suggesting improvements, opening issues, or submitting pull requests, please feel free to do so. Your input is valuable in making this resource even better.
